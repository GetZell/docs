---
title: "How to Create a Scorecard"
description: "Learn how to create and configure scorecards to evaluate team performance"
icon: "star"
---

## Overview

Scorecards help you systematically evaluate calls, emails, and interactions based on custom criteria. This guide walks you through creating an effective scorecard for your team.

## Steps to Create a Scorecard

<Steps>
  <Step title="Navigate to Scorecards">
    Go to **Team Management** â†’ **Scorecards**.
  </Step>

  <Step title="Start a New Scorecard">
    Click **Create New** to start from scratch, or choose **Duplicate** to clone an existing scorecard.

    <Tip>Duplicating an existing scorecard is a quick way to create variations for different teams or use cases.</Tip>
  </Step>

  <Step title="Duplicate (Optional)">
    If duplicating:
    - Select the scorecard you want to copy
    - Click **Duplicate**
    - Click **Open** on the new copy
  </Step>

  <Step title="Rename Your Scorecard">
    Change the title to something descriptive that reflects the scorecard's purpose (e.g., "Sales Call Evaluation" or "Customer Support Quality Check").
  </Step>

  <Step title="Add Categories">
    For each skill or stage you want to evaluate:
    - Click **Add Category**
    - Enter a clear category name (e.g., "Objection Handling", "Opening Statement", "Closing Technique")

    <Note>Categories help organize your evaluation criteria and make scorecards easier to use.</Note>
  </Step>

  <Step title="Define Evaluation Criteria">
    Under each category, write a concise rule that the AI model or reviewer will check.

    **Example:**
    > "Did the rep rephrase the objection and address it?"

    Keep criteria specific and measurable for consistent evaluations.
  </Step>

  <Step title="Provide User Guidance">
    In the "Explanation" or "User Guide" field, give clear instructions on how to meet the criteria.

    **Example:**
    > "When an objection arises, first acknowledge it, then restate it in your own words, and finally provide a clear resolution or alternative perspective."

    This helps team members understand what's expected.
  </Step>

  <Step title="Include Examples">
    Paste sample scenarios or dialogue snippets to illustrate:
    - **Good performance:** What a successful interaction looks like
    - **Poor performance:** Common mistakes to avoid

    Examples help both AI evaluators and human reviewers maintain consistency.
  </Step>

  <Step title="Save Your Scorecard">
    Click **Save** to finalize your scorecard configuration.
  </Step>

  <Step title="Assign & Test">
    - Attach the new scorecard to calls or training sessions
    - Run a demo evaluation to ensure your categories and criteria work as intended
    - Make adjustments based on the test results

    <Tip>Start with a small test group before rolling out to your entire team.</Tip>
  </Step>
</Steps>

## Best Practices

<CardGroup cols={2}>
  <Card title="Be Specific" icon="bullseye">
    Write clear, measurable criteria that leave little room for interpretation.
  </Card>

  <Card title="Use Examples" icon="lightbulb">
    Provide concrete examples of both good and poor performance for each criterion.
  </Card>

  <Card title="Test First" icon="flask">
    Always test your scorecard on sample interactions before deploying widely.
  </Card>

  <Card title="Iterate" icon="rotate">
    Regularly review and refine your scorecards based on team feedback and results.
  </Card>
</CardGroup>